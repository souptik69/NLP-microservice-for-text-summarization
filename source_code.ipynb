{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssen\\Documents\\syenah\\nlp-microservice-trojkn\\NLPtest\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "# import transformers\n",
    "# import tensorflow\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client.myDatabase\n",
    "articles_collection = db.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ssen\\Documents\\syenah\\nlp-microservice-trojkn\\NLPtest\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ssen\\Documents\\syenah\\nlp-microservice-trojkn\\NLPtest\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssen\\Documents\\syenah\\nlp-microservice-trojkn\\NLPtest\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ssen\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "c:\\Users\\ssen\\Documents\\syenah\\nlp-microservice-trojkn\\NLPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"tf\")\n",
    "class ArticleSourceLocation(BaseModel):\n",
    "    country: str\n",
    "    state: str\n",
    "    city: str\n",
    "    coordinates: Dict[str, float]\n",
    "\n",
    "class ArticleSource(BaseModel):\n",
    "    domain: str\n",
    "    location: ArticleSourceLocation\n",
    "\n",
    "class Article(BaseModel):\n",
    "    uri: str\n",
    "    title: str\n",
    "    body: str\n",
    "    publication_datetime: str\n",
    "    lang: str\n",
    "    url: str\n",
    "    source: ArticleSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/summarize_article\")\n",
    "async def summarize_article(article: Article):\n",
    "    # Summarize the article\n",
    "    summary = summarizer(article.body, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Prepare the article document\n",
    "    article_doc = {\n",
    "        \"uri\": article.uri,\n",
    "        \"title\": article.title,\n",
    "        \"body\": article.body,\n",
    "        \"publication_datetime\": article.publication_datetime,\n",
    "        \"lang\": article.lang,\n",
    "        \"url\": article.url,\n",
    "        \"source\": {\n",
    "            \"domain\": article.source.domain,\n",
    "            \"location\": {\n",
    "                \"country\": article.source.location.country,\n",
    "                \"state\": article.source.location.state,\n",
    "                \"city\": article.source.location.city,\n",
    "                \"coordinates\": {\n",
    "                    \"lat\": article.source.location.coordinates['lat'],\n",
    "                    \"lon\": article.source.location.coordinates['lon']\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    \n",
    "    # Store the document in MongoDB\n",
    "    result = articles_collection.update_one(\n",
    "        {\"uri\": article.uri},\n",
    "        {\"$set\": article_doc},\n",
    "        upsert=True\n",
    "    )\n",
    "    \n",
    "    if result.upserted_id or result.modified_count:\n",
    "        return {\"uri\": article.uri, \"summary\": summary}\n",
    "    else:\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to save the article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/result/{uri}\")\n",
    "async def get_result(uri: str):\n",
    "    # Retrieve the summarized article from MongoDB\n",
    "    article = articles_collection.find_one({\"uri\": uri})\n",
    "    \n",
    "    if article:\n",
    "        return {\n",
    "            \"uri\": article[\"uri\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"summary\": article[\"summary\"]\n",
    "        }\n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Article not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
